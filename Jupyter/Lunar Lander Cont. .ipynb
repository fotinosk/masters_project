{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c100e9dccf85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mddpg_agent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mddpg_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCriticNetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActorNetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplaybuffer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementation of DDPG - Deep Deterministic Policy Gradient Algorithm and hyperparameter details can be found here:\n",
    "    http://arxiv.org/pdf/1509.02971v2.pdf\n",
    "The algorithm is tested on the Pendulum-v0 and MountainCarContinuous-v0 OpenAI gym task\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.agent.ddpg_agent import DDPGAgent\n",
    "from src.network.ddpg_network import CriticNetwork, ActorNetwork\n",
    "from src.replaybuffer import ReplayBuffer\n",
    "from src.explorationnoise import OrnsteinUhlenbeckProcess, GreedyPolicy\n",
    "\n",
    "flags = tf.app.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of DDPG - Deep Deterministic Policy Gradient Algorithm and hyperparameter details can be found here:\n",
    "    http://arxiv.org/pdf/1509.02971v2.pdf\n",
    "The algorithm is tested on the Pendulum-v0 and MountainCarContinuous-v0 OpenAI gym task\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.agent.ddpg_agent import DDPGAgent\n",
    "from src.network.ddpg_network import CriticNetwork, ActorNetwork\n",
    "from src.replaybuffer import ReplayBuffer\n",
    "from src.explorationnoise import OrnsteinUhlenbeckProcess, GreedyPolicy\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "# ================================\n",
    "#    UTILITY PARAMETERS\n",
    "# ================================\n",
    "# Gym environment name\n",
    "#'Pendulum-v0''MountainCarContinuous-v0'\n",
    "flags.DEFINE_string('env_name', 'Pendulum-v0', 'environment name in gym.')\n",
    "flags.DEFINE_boolean('env_render', False, 'whether render environment (display).')\n",
    "flags.DEFINE_boolean('env_monitor', True, 'whether use gym monitor.')\n",
    "DATETIME = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "RANDOM_SEED = 1234\n",
    "\n",
    "\n",
    "# ================================\n",
    "#    TRAINING PARAMETERS\n",
    "# ================================\n",
    "flags.DEFINE_integer('mini_batch', 64, 'mini batch size for training.')\n",
    "# Learning rates actor and critic\n",
    "ACTOR_LEARNING_RATE = 0.0001\n",
    "CRITIC_LEARNING_RATE = 0.001\n",
    "# Maximum number of episodes\n",
    "MAX_EPISODES = 100000\n",
    "# Maximum number of steps per episode\n",
    "MAX_STEPS_EPISODE = 50000\n",
    "# warmup steps.\n",
    "WARMUP_STEPS = 10000\n",
    "# Exploration duration\n",
    "EXPLORATION_EPISODES = 10000\n",
    "# Discount factor\n",
    "GAMMA = 0.99\n",
    "# Soft target update parameter\n",
    "TAU = 0.001\n",
    "# Size of replay buffer\n",
    "BUFFER_SIZE = 1000000\n",
    "# Exploration noise variables Ornstein-Uhlenbeck variables\n",
    "OU_THETA = 0.15\n",
    "OU_MU = 0.\n",
    "OU_SIGMA = 0.3\n",
    "# Explorationnoise for greedy policy\n",
    "MIN_EPSILON = 0.1\n",
    "MAX_EPSILON = 1\n",
    "\n",
    "#================\n",
    "# parameters for evaluate.\n",
    "#================\n",
    "# evaluate periods\n",
    "EVAL_PERIODS = 100\n",
    "# evaluate episodes\n",
    "EVAL_EPISODES = 10\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Directory for storing gym results\n",
    "MONITOR_DIR = './results/{}/{}/gym_ddpg'.format(FLAGS.env_name, DATETIME)\n",
    "# Directory for storing tensorboard summary results\n",
    "SUMMARY_DIR = './results/{}/{}/tf_ddpg'.format(FLAGS.env_name, DATETIME)\n",
    "\n",
    "\n",
    "# ================================\n",
    "#    MAIN\n",
    "# ================================\n",
    "def main(_):\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        env = gym.make(FLAGS.env_name)\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "        env.seed(RANDOM_SEED)\n",
    "\n",
    "        if FLAGS.env_monitor:\n",
    "            if not FLAGS.env_render:\n",
    "                env = Monitor(env, MONITOR_DIR, video_callable=False, force=True)\n",
    "            else:\n",
    "                env = Monitor(env, MONITOR_DIR, force=True)\n",
    "\n",
    "        state_dim = env.observation_space.shape\n",
    "        try: \n",
    "            action_dim = env.action_space.shape[0]\n",
    "            action_bound = env.action_space.high\n",
    "            # Ensure action bound is symmetric\n",
    "            assert(np.all(env.action_space.high == -env.action_space.low))\n",
    "            action_type = 'Continuous'\n",
    "        except:\n",
    "            action_dim = env.action_space.n\n",
    "            action_bound = None\n",
    "            action_type = 'Discrete'\n",
    "\n",
    "        print(action_type)\n",
    "        actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                             ACTOR_LEARNING_RATE, TAU, action_type)\n",
    "\n",
    "        critic = CriticNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                               CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars(), action_type)\n",
    "\n",
    "        # Initialize replay memory\n",
    "        replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "        if action_type == 'Continuous':\n",
    "            noise = OrnsteinUhlenbeckProcess(OU_THETA, mu=OU_MU, sigma=OU_SIGMA, n_steps_annealing=EXPLORATION_EPISODES)\n",
    "        else:\n",
    "            noise = GreedyPolicy(action_dim, EXPLORATION_EPISODES, MIN_EPSILON, MAX_EPSILON)\n",
    "\n",
    "\n",
    "        agent = DDPGAgent(sess, action_type, actor, critic, GAMMA, env, replay_buffer, noise=noise, exploration_episodes=EXPLORATION_EPISODES,\\\n",
    "                max_episodes=MAX_EPISODES, max_steps_episode=MAX_STEPS_EPISODE, warmup_steps=WARMUP_STEPS,\\\n",
    "                mini_batch=FLAGS.mini_batch, eval_episodes=EVAL_EPISODES, eval_periods=EVAL_PERIODS, \\\n",
    "                env_render=FLAGS.env_render, summary_dir=SUMMARY_DIR)\n",
    "\n",
    "        agent.train()\n",
    "        env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
