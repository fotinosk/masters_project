{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Guide to Building a Deep Deterministic Policy Gradient Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Original Paper from DeepMind"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Overview:\n",
    "\n",
    "- Actor-Critic Architecture\n",
    "- Model-free\n",
    "- Need discrete time steps\n",
    "- Assumes the enviroment is fully observed\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "x: Observations (state)\n",
    "\n",
    "a: Action\n",
    "\n",
    "t: Reward\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Agents behaviour is determined by the policy (pi)\n",
    "\n",
    "The policy maps states into a probability distribution of actions\n",
    "\n",
    "(Enviromet may be stochastic)\n",
    "\n",
    "Modeled as a Markov Decision process with transition dynamics: p(s_(t+1)|s_(t),a_(t))\n",
    "\n",
    "The return from a state is defined as the sum of discounted rewards:\n",
    "\n",
    "    R_t = sum(gamma^(i-t) * r(s_i, a_i))\n",
    "\n",
    "        Where gamma in [0,1]: discount factor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Goal: Learn a policy that maximizes the expecter reward from the statr distribution\n",
    "\n",
    "    J = Exp{R_1}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Action-value function: Describes the expected return after taking an action in a given state, following the policy policy\n",
    "\n",
    "    Q^(pi)(s_t,a_t) = Exp{R_t|s_t, a_t}\n",
    "\n",
    "From the Bellman Equation:\n",
    "\n",
    "    Q^(pi)(s_t,a_t) = Exp{r(s_t,a_t) + gamma * Exp{Q^(pi)(s_t+1,a_t+1)}}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Agent's Policy: pi: S -> A\n",
    "\n",
    "Target Policy:  mu: S <- A (if deterministic)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Target Policy\n",
    "\n",
    "    Q^(mu)(s_t,a_t) = Exp(r(s_t,a_t) + gamma * Q^(mu)(s_t+1, mu(s_t+1)))\n",
    "\n",
    "    ie a_t+1 = mu(s_t+1)\n",
    "\n",
    "Note that in the above equation the second expectation was removed since the process is now deterministic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Greedy Policy (For the Actor):\n",
    "\n",
    "    mu(s) = arg_max_a Q(s,a)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loss Function\n",
    "\n",
    "    L(theta^Q) = Exp{(Q(s_t, a_t | theta^Q) - y_t)^2}\n",
    "\n",
    "    Where y_t = r(s_t,a_t) + gamma * Q(s_t+1, mu(s_t+1) | theta^Q)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### theta^Q are the parameters to be optimized"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Actor:  mu(s|theta^mu)\n",
    "\n",
    "Critic: Q(s,a)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Updating Agents\n",
    "\n",
    "Critic: Updated as dictated by the Bellman Equation\n",
    "\n",
    "Actor: By applying the chain rule to the expected return from the start distn J wrt the actor parameters, theta^mu\n",
    "\n",
    "\n",
    "\n",
    "    Grad(J) ~ Exp{Grad_(theta^mu)(Q(s,a|theta^Q))}, at s=s_t, a=mu(s_t|theta^mu)\n",
    "\n",
    "            = Exp{Grad_(a)Q(s,a|theta^Q)) * Grad_(theta^mu)(mu(s|theta^mu))}, at s=s_t a=mu(s_t)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Note: Non-linear function approximators don't guarantee convergence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Replay Buffer\n",
    "\n",
    "- Finite sized cache, R\n",
    "- Transitions are sampled form the enviroment according to an exploration policy\n",
    "- Transition tuple, (s_t, a_t, s_t+1, a_t+1), is stored in the Replay Buffer\n",
    "- The buffer is FIFO, and oldest samples are discarded\n",
    "- At each timestep the actor and the critic is updated by sampling a minibatch uniformly from the buffer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Problem:    The critic network being updated is also used to the calculate the target value, making it potentially unstable\n",
    "\n",
    "Solution:   Implementation of a Target network that uses soft target updates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Target Networks\n",
    "\n",
    "- Create a copy of the of the actor and critic Networks, Q'(s,a|theta^Q') and mu'(s|theta^mu') respectively\n",
    "- Those will now be used to calculate the target values\n",
    "- The weights of these networks are slowly updated by having them slowly track the learned networks "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Target Network Updates\n",
    "\n",
    "    theta' = tau * theta + (1-tau) * theta'\n",
    "\n",
    "This applies to both target networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Problem: Different components of the observation may have different physical units, making it difficult for the network to learn effectively\n",
    "\n",
    "Solution: Batch Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Batch Normalization\n",
    "\n",
    "- Normalizes each dimension across the samples in a minibatch to have unit mean and variance\n",
    "- Keeps the running average of the mean and variance to use for nornamization during testing\n",
    "- Used on state input, all layers of the mu network, all layers of the Q network before the action input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exploration\n",
    "\n",
    "- Exploration policy is contructed by the addition of noise to the actor policy\n",
    "\n",
    "        mu'(s_t) = mu(s_t|theta_t^(mu)) + Noise\n",
    "\n",
    "- OU noise is recomended\n",
    "\n",
    "- When the training is running the policy is periodically evaluated without the exploration noise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Implementation Details\n",
    "\n",
    "- lr_actor  = 10^-4\n",
    "- lr_critic = 10^-3\n",
    "- For Q, L_2 weight decay of 10^-2 was used\n",
    "- gamma = 0.99\n",
    "- tau = 0.001\n",
    "\n",
    "\n",
    "- The neural networs used the rectified non-linearity for all hidden layers\n",
    "- Final output layer for actor, tanh layer, to bound the actions between [-1,1]\n",
    "- Low-dim networs have 2 hidden layers with size [400,300]\n",
    "\n",
    "\n",
    "- Actions are included only in the 2nd hidden layer of Q\n",
    "- The final layer weights and biases of both actor and critic are initialized from a uniform distribution [-3x10^_3, 3x10^_3]\n",
    "- Other layers were initialized from uniform distributions [-1/sqrt(f), 1/sqrt(f)]\n",
    "    - f is the fan-in of the layer\n",
    "\n",
    "\n",
    "- Minibatch size = 64\n",
    "- Replay buffer size = 10^6\n",
    "\n",
    "\n",
    "- OU noise used with theta=0.15 and sigma=0.2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Components\n",
    "\n",
    "- Actor (and Target Actor)\n",
    "- Critic (and Target Critic)\n",
    "- Replay Buffer\n",
    "- Noise\n",
    "- Action Normalization (optional?) (maybe not optional since tanh is used for output layer)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Processes\n",
    "\n",
    "- Forward Propagation\n",
    "- Backward Propagation (Network Updates)\n",
    "- Batch Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Replay Buffer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'done', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, size, mini_batch_size):\n",
    "        self.size = size\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, *args):\n",
    "        \"\"\"Add transition to buffer\"\"\"\n",
    "        \n",
    "        # This is a strange way to implement it, but makes it FIFO efficiently\n",
    "        if len(self.memory) < self.size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = int((self.position + 1) % self.size)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Get a minibatch from buffer\"\"\"\n",
    "        return random.sample(self.memory, self.mini_batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Memory buffer used for learning, takes in a tuple: ('state', 'action', 'done', 'next_state', 'reward')\"\n"
   ]
  },
  {
   "source": [
    "# UO Action Noise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Here there is a decision to be made between action and parameter noise\n",
    "# https://openai.com/blog/better-exploration-with-parameter-noise/\n",
    "# The original DDPG paper seems to suggest using action noise\n",
    "# Ornstein-Uhlenbeck noise used since it is initially correlated\n",
    "\n",
    "class ActionNoise:\n",
    "\n",
    "    def __init__(self, mu, theta=0.15, sigma=0.2, x0=None, dt=0.05):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.mu = mu # will be initialized as a list of zeros\n",
    "        self.x0 = x0\n",
    "        self.dt = dt # same as flight model\n",
    "\n",
    "        self.reset() # sets x_prev\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "source": [
    "# Fan-in Initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def fan_in_init(tensor, fan_in=None):\n",
    "    # Either of the above inputs works\n",
    "\n",
    "    if fan_in is None:\n",
    "        fan_in = tensor.size(-1)\n",
    "\n",
    "    w = 1./ np.sqrt(fan_in)\n",
    "    return nn.init.uniform_(tensor, -w, w)"
   ]
  },
  {
   "source": [
    "# Actor Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, action_space, init_w = 3e-3, hidden_1=400, hidden_2=300, init_b=3e-4):\n",
    "        super(Actor, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        self.num_outputs = action_space.shape[0]\n",
    "\n",
    "        # Build the architecture of the actor\n",
    "        # Investigate using LayerNorm vs BatchNorm, it seems they are the same, how??\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_1)\n",
    "        # self.fcn1 = nn.BatchNorm1d(hidden_1)\n",
    "        self.fcn1 = nn.LayerNorm(hidden_1)\n",
    "\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # self.fcn2 = nn.BatchNorm1d(hidden_2)\n",
    "        self.fcn2 = nn.LayerNorm(hidden_2)\n",
    "\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_2, self.num_outputs)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "        # Initialize weights\n",
    "        # All layers except last are initialized with fan in method\n",
    "        self.fc1.weight.data = fan_in_init(self.fc1.weight)\n",
    "        self.fc1.bias.data = fan_in_init(self.fc1.bias)\n",
    "\n",
    "        self.fc2.weight.data = fan_in_init(self.fc2.weight)\n",
    "        self.fc2.bias.data = fan_in_init(self.fc2.bias)\n",
    "\n",
    "        # The final layer weights and biases were initialized from uniform [-3e-3, 3e-3]\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_b, init_b)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.fc1(inputs)\n",
    "        out = self.fcn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.fcn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh(out)\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "### LayerNorm is used for now, but might consider changing in the future"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Critic Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, action_space,init_w=3e-3, hidden_1=400, hidden_2=300, init_b=3e-4):\n",
    "        super(Critic, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.action_space = action_space\n",
    "        self.num_outputs = action_space.shape[0]\n",
    "\n",
    "        # Build the architecture of the actor\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_1)\n",
    "        self.fcn1 = nn.LayerNorm(hidden_1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_1+self.num_outputs, hidden_2)\n",
    "        self.fcn2 = nn.LayerNorm(hidden_2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1.weight.data = fan_in_init(self.fc1.weight)\n",
    "        self.fc1.bias.data = fan_in_init(self.fc1.bias)\n",
    "\n",
    "        self.fc2.weight.data = fan_in_init(self.fc2.weight)\n",
    "        self.fc2.bias.data = fan_in_init(self.fc2.bias)\n",
    "\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_b, init_b)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = state\n",
    "        x = self.fc1(x)\n",
    "        x = self.fcn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(torch.cat([x,action],1))\n",
    "        x = self.fcn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        out = self.fc3(x)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "# Normalized Actions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized actions\n",
    "\n",
    "import gym\n",
    "\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/2\n",
    "        act_b = (self.action_space.high + self.action_space.low)/2\n",
    "\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self,action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b_inv = (self.action_space.high + self.action_space.low)/2\n",
    "        return act_k_inv * (action - act_b_inv)"
   ]
  },
  {
   "source": [
    "# Network Updates\n",
    "\n",
    "Hard: Copies the weights of one Network to another\n",
    "\n",
    "Soft: Partially updates the weights based in the difference in weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "source": [
    "A lot of users seem to not be implementing the L2 weight decay\n",
    "\n",
    "Should only be implemented in the ciritc\n",
    "\n",
    "Implment in linear layers or in optimizer (?)\n",
    "\n",
    "They seem to be more or less the same thing, so optimizer weight decay is used, which is simpler to implemente\n",
    "\n",
    "Or is it? If it's in the optimizer, then it is not reflected in the target ---- What does this mean???"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# DDPG\n",
    "This is the agent that brings it all together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import os\n",
    "\n",
    "class DDPG(object):\n",
    "\n",
    "    def __init__(self, num_inputs, action_space, checkpoint_dir=None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = action_space.shape[0]\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr_actor = 10e-4\n",
    "        self.lr_critic = 10e-3\n",
    "        self.buffer_size = 10e6\n",
    "        self.batch_size = 64\n",
    "        self.noise_mean = np.zeros(self.num_outputs)\n",
    "        self.tau = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.weight_decay = 0.01\n",
    "\n",
    "        # create actor critic networks\n",
    "        self.actor = Actor(self.num_inputs, action_space)\n",
    "        self.critic = Critic(self.num_inputs, action_space)\n",
    "        \n",
    "        # create target networks\n",
    "        self.target_actor = Actor(self.num_inputs, action_space)\n",
    "        self.target_critic = Critic(self.num_inputs, action_space)\n",
    "\n",
    "        # ensure that the weights of the targets are the same as the actor critic\n",
    "        hard_update(self.target_actor, self.actor)\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "\n",
    "        # set up the optimizers\n",
    "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=self.lr_critic, weight_decay=self.weight_decay)\n",
    "\n",
    "        # create replay buffer and noise\n",
    "        self.buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "        self.noise = ActionNoise(self.noise_mean)\n",
    "\n",
    "        # Set the directory to save the models\n",
    "        if checkpoint_dir is None:\n",
    "            self.checkpoint_dir = \"./saves/\"\n",
    "        else:\n",
    "            self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        # Update the model paremeters by sampling the memory buffer\n",
    "        batch = Transition(*zip(*self.buffer.sample()))\n",
    "\n",
    "        state_batch = torch.cat(batch.state).float()\n",
    "        action_batch = torch.cat(batch.action).float()\n",
    "        reward_batch = torch.cat(batch.reward).float()\n",
    "        done_batch = torch.cat(batch.done).float()\n",
    "        next_state_batch = torch.cat(batch.next_state).float()\n",
    "\n",
    "        # Using the target networks calculate the actions and values\n",
    "        next_action_batch = self.target_actor(next_state_batch)\n",
    "        next_qs = self.target_critic(next_state_batch, next_action_batch.detach())  # Not sure why detach is used here\n",
    "\n",
    "        # computations\n",
    "        reward_batch = reward_batch.unsqueeze(1)\n",
    "        done_batch = done_batch.unsqueeze(1)\n",
    "        exp_values = reward_batch + (1- done_batch) * self.gamma * next_qs\n",
    "\n",
    "        # critic update\n",
    "        self.critic_optim.zero_grad()\n",
    "        state_action_batch = self.critic(state_batch, action_batch)\n",
    "        value_loss = F.mse_loss(state_action_batch, exp_values.detach())\n",
    "        value_loss.backward()\n",
    "        #DQN uses clamp step here\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # actor update\n",
    "        self.actor_optim.zero_grad()\n",
    "        policy_loss = - self.critic(state_batch, self.actor(state_batch))\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # update target networks\n",
    "        soft_update(self.target_actor, self.actor, self.tau)\n",
    "        soft_update(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "        return value_loss.item(), policy_loss.item()\n",
    "\n",
    "    def get_action(self, state, add_noise=True):\n",
    "        # one example I found used episode decay, I couldn't find any other case where this is used, so i ignored it\n",
    "        # it is used in dqn tho\n",
    "        # state = torch.from_numpy(state).float()\n",
    "        self.actor.eval()  # puts actor into evaluation mode, ie not training any more, this means for eg that dropout layers dont dropout etc\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up …\n",
    "            mu = self.actor(state).data\n",
    "\n",
    "        self.actor.train() # return actor to train mode, undos eval mode\n",
    "\n",
    "        if add_noise:\n",
    "            mu += self.noise()\n",
    "        # return np.clip(mu, -1 ,1)\n",
    "        return mu.clamp(self.action_space.low[0], self.action_space.high[0])\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(-1,1, self.num_inputs)\n",
    "        return action\n",
    "\n",
    "    def set_eval(self):\n",
    "        # set all agents to evaluation mode\n",
    "        self.actor.eval()\n",
    "        self.critic.eval()\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "\n",
    "    def set_train(self):\n",
    "        # set all agents to training mode\n",
    "        self.actor.train()\n",
    "        self.critic.train()\n",
    "        self.target_actor.train()\n",
    "        self.target_critic.train()\n",
    "\n",
    "    def save(self, last_time):\n",
    "        save_path = self.checkpoint_dir + f'/ep{last_time}.pth.tar'\n",
    "        print('Saving...')\n",
    "        checkpoint = {\n",
    "            'last_timestep': last_time,\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'actor_target': self.target_actor.state_dict(),\n",
    "            'critic_target': self.target_critic.state_dict(),\n",
    "            'actor_optim': self.actor_optim.state_dict(),\n",
    "            'critic_optim': self.critic_optim.state_dict(),\n",
    "            'memory': self.memory\n",
    "        }\n",
    "        torch.save(checkpoint, save_path)\n",
    "        # Garbage collection, reclaims some memory\n",
    "        gc.collect()\n",
    "        print(f\"Model saved: {last_time},  {save_path}\")\n",
    "\n",
    "    def load(self, path=None):\n",
    "        # Loads checkpoint\n",
    "        if path is None:\n",
    "            path = self.get_path()\n",
    "        \n",
    "        if os.path.isfile(path):\n",
    "            print(\"Loading checkpoint...\")\n",
    "\n",
    "        checkpoint = torch.load(path)\n",
    "        timestep = checkpoint['last_timestep'] + 1\n",
    "\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.target_actor.load_state_dict(checkpoint['actor_target'])\n",
    "        self.target_critic.load_state_dict(checkpoint['critic_target'])\n",
    "        self.actor_optim.load_state_dict(checkpoint['actor_optim'])\n",
    "        self.critic_optim.load_state_dict(checkpoint['critic_optim'])\n",
    "        replay_buffer = checkpoint['memory']\n",
    "\n",
    "        gc.collect()\n",
    "        print('Model Loaded')\n",
    "        return timestep, replay_buffer\n",
    "\n",
    "    def get_path(self):\n",
    "        # Gets the path of the latest file\n",
    "        files = [file for file in os.listdir(self.checkpoint_dir) if (file.endswith(\".pt\") or file.endswith(\"tar\"))]\n",
    "        path = [os.path.join(self.checkpoint_dir, file) or file in files]\n",
    "        last_file = max(path, key=os.path.getctime)\n",
    "        return os.path.abspath(last_file)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Training and Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_Boeing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('boeing-danger-v0')\n",
    "# TODO: It seems that agent doesnt work well with normalized action (why and how it works)\n",
    "# env = NormalizedEnv(env)\n",
    "%matplotlib auto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPG(3, action_space=env.action_space)"
   ]
  },
  {
   "source": [
    "Warmup steps are added, the agent picks random actions at first when training, to encourage exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "n_test_cycles = 10\n",
    "warmup = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# set the networks in training mode\n",
    "agent.set_train()\n",
    "\n",
    "timestep = 1\n",
    "rewards, policy_losses, value_losses, mean_test_rewards = [], [], [], []\n",
    "epoch = 0\n",
    "t = 0\n",
    "time_last_checkpoint = time.time()\n",
    "\n",
    "while timestep <= 100:\n",
    "    agent.noise.reset()\n",
    "    epoch_return = 0.\n",
    "    state = torch.Tensor([env.reset()])\n",
    "\n",
    "    while True:\n",
    "        # TODO : sampled action may not be normalized\n",
    "        # if epoch == 0 and timestep < warmup:\n",
    "        #     action = env.action_space.sample()\n",
    "        #     action = torch.Tensor([action])\n",
    "        # else:\n",
    "        #      action = agent.get_action(state)\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.numpy()[0])\n",
    "        print(done,reward, _)\n",
    "        timestep += 1\n",
    "        epoch_return += reward\n",
    "        \n",
    "        mask = torch.Tensor([done])\n",
    "        reward = torch.Tensor([reward])\n",
    "        next_state = torch.Tensor([next_state])\n",
    "\n",
    "        agent.buffer.add(state, action, mask, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        epoch_value_loss = 0\n",
    "        epoch_policy_loss = 0\n",
    "\n",
    "        # TODO: only update if the warmup period is over?\n",
    "        if len(agent.buffer) > agent.buffer.mini_batch_size:\n",
    "            value_loss, policy_loss = agent.update()\n",
    "\n",
    "            epoch_value_loss += value_loss\n",
    "            epoch_policy_loss += policy_loss\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # TODO: implement max ep len here??\n",
    "\n",
    "    \n",
    "    rewards.append(epoch_return)\n",
    "    value_losses.append(epoch_value_loss)\n",
    "    policy_losses.append(epoch_policy_loss)\n",
    "\n",
    "    if timestep >= 10 * t:\n",
    "        # One epoch has passed and it's time to present some results to the user\n",
    "        print('Epoch:', epoch)\n",
    "        t += 1\n",
    "        test_rewards = []\n",
    "\n",
    "        # for results to be generated, the agent is run without exploration noise\n",
    "        for _ in range(n_test_cycles):\n",
    "            state = torch.Tensor(env.reset())\n",
    "            test_reward = 0\n",
    "            while True:\n",
    "                # this is a bit different form the implementation used above, although it does the same job\n",
    "                # this is due to a bug that instead of returning action:[[]], returns action:[] needing for\n",
    "                # the action to be reshaped\n",
    "                action = agent.get_action(state, add_noise=False)\n",
    "                action = action.numpy()\n",
    "                action = action.reshape((2,))\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                print(done,_)\n",
    "                test_reward += reward\n",
    "                next_state = torch.Tensor([next_state])\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            test_rewards.append(test_reward)\n",
    "        mean_test_rewards.append(np.mean(test_reward))\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "\n",
    "    # save model\n",
    "    agent.save(timestep, agent.buffer.memory)\n",
    "    env.close()"
   ]
  },
  {
   "source": [
    "# # alternative set of hyperparameters\n",
    "# num_iternations = 200000\n",
    "# warmup_steps = 1000\n",
    "# max_ep_len = 2000\n",
    "# validate_steps = 2000  # how often to perfrom evaluations\n",
    "\n",
    "# agent.set_train()\n",
    "# episode = episode_steps = 0\n",
    "# step = 1\n",
    "# episode_reward = 0.\n",
    "# state = None\n",
    "\n",
    "# while step < num_iternations:\n",
    "#     if state is None:\n",
    "#         state = torch.Tensor([env.reset()])\n",
    "\n",
    "#     if step <= warmup_steps:\n",
    "#         action = env.action_space.sample()\n",
    "#         action = torch.Tensor([action])\n",
    "#     else:\n",
    "#         action = agent.get_action(state)\n",
    "\n",
    "#     next_state, reward, done, _ = env.step(action.numpy()[0])\n",
    "\n",
    "#     if max_ep_len and episode_steps >= max_ep_len -1:\n",
    "#         done = True\n",
    "\n",
    "#     mask = torch.Tensor([done])\n",
    "#     reward = torch.Tensor([reward])\n",
    "#     next_state = torch.Tensor([next_state])\n",
    "\n",
    "#     agent.buffer.add(state, action, mask, next_state, reward)\n",
    "#     state = next_state\n",
    "\n",
    "#     epoch_value_loss = 0\n",
    "#     epoch_policy_loss = 0\n",
    "\n",
    "#     if step > warmup_steps and len(agent.buffer) > agent.buffer.mini_batch_size:\n",
    "#         value_loss, policy_loss = agent.update()\n",
    "#         epoch_value_loss += value_loss\n",
    "#         epoch_policy_loss += policy_loss\n",
    "\n",
    "#     # evaluate TODO: can be better done using a class\n",
    "#     if step % validate_steps == 0:\n",
    "#         print('Now in Validation Mode')\n",
    "#         state = torch.Tensor([env.reset()])\n",
    "#         test_reward = 0\n",
    "#         validation_ep = 0\n",
    "#         while True:\n",
    "#             action = agent.get_action(state, add_noise=False)\n",
    "#             action = action.numpy()\n",
    "#             action = action.reshape((2,))\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "#             test_reward += reward\n",
    "#             next_state = torch.Tensor([next_state])\n",
    "#             state = next_state\n",
    "#             if validation_ep == max_ep_len:\n",
    "#                 done = True\n",
    "#             if done:\n",
    "#                 break\n",
    "#             validation_ep += 1\n",
    "#         print(step, test_reward)          \n",
    "\n",
    "#     step += 1\n",
    "#     episode_steps += 1\n",
    "#     episode_reward += reward\n",
    "\n",
    "#     if done: \n",
    "#         print(f\"Done, step: {step}, episode: {episode}, episode steps: {episode_steps}, {epoch_policy_loss}, {epoch_value_loss}\")\n",
    "#         env.render()\n",
    "#         # reset\n",
    "#         state = None\n",
    "#         episode_steps = 0\n",
    "#         episode_reward = 0. \n",
    "#         episode += 1\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}