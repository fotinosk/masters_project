{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Recurent Deterministic Policy Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "RDPG is a natural extension of the DDPG, where an some recurrency is added to the model, so that it can use and adjust using past events. \n",
    "\n",
    "Especially beneficial in the case of partially observed enviroments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The changes from the DDPG architecture seem to limit themselves to the memory buffer and the actor-critic architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Action Noise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActionNoise:\n",
    "\n",
    "    def __init__(self, mu, theta=0.15, sigma=0.2, x0=None, dt=0.05):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.mu = mu  # will be initialized as a list of zeros\n",
    "        self.x0 = x0\n",
    "        self.dt = dt  # same as flight model\n",
    "\n",
    "        self.reset()  # sets x_prev\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(\n",
    "            self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "source": [
    "## Replay Buffer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'done', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, size, mini_batch_size):\n",
    "        self.size = size\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, *args):\n",
    "        \"\"\"Add transition to buffer\"\"\"\n",
    "\n",
    "        # This is a strange way to implement it, but makes it FIFO efficiently\n",
    "        if len(self.memory) < self.size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = int((self.position + 1) % self.size)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Get a minibatch from buffer\"\"\"\n",
    "        return random.sample(self.memory, self.mini_batch_size)\n",
    "\n",
    "    def sample_trajectory(self):\n",
    "        \"\"\"Get a random batch of consecutive results\"\"\"\n",
    "        start = random.randint(0, self.size-self.mini_batch_size)\n",
    "        return self.memory[start :start+self.mini_batch_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Memory buffer used for learning, takes in a tuple: ('state', 'action', 'done', 'next_state', 'reward')\"\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Fan-In Initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def fan_in_init(tensor, fan_in=None):\n",
    "    # Either of the above inputs works\n",
    "\n",
    "    if fan_in is None:\n",
    "        fan_in = tensor.size(-1)\n",
    "\n",
    "    w = 1. / np.sqrt(fan_in)\n",
    "    return nn.init.uniform_(tensor, -w, w)"
   ]
  },
  {
   "source": [
    "## Enviroment Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def step(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low) / 2\n",
    "        act_b = (self.action_space.high + self.action_space.low) / 2\n",
    "\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2. / (self.action_space.high - self.action_space.low)\n",
    "        act_b_inv = (self.action_space.high + self.action_space.low) / 2\n",
    "        return act_k_inv * (action - act_b_inv)"
   ]
  },
  {
   "source": [
    "## Actor Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self,num_inputs, action_space, init_w=3e-3, init_b=3e-4, hidden1=400, hidden2=300):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.num_ouptuts = action_space.shape[0]\n",
    "\n",
    "        # original model has lstm after second layer, but i think it might work better between hidden1 and hidden2\n",
    "\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden1)\n",
    "        self.fcn1 = nn.LayerNorm(hidden1)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(hidden1, hidden1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fcn2 = nn.LayerNorm(hidden2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden2, self.num_ouptuts)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.fc1.weight.data = fan_in_init(self.fc1.weight)\n",
    "        self.fc1.bias.data = fan_in_init(self.fc1.bias)\n",
    "\n",
    "        self.fc2.weight.data = fan_in_init(self.fc2.weight)\n",
    "        self.fc2.bias.data = fan_in_init(self.fc2.bias)\n",
    "\n",
    "        # The final layer weights and biases were initialized from uniform [-3e-3, 3e-3]\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_b, init_b)\n",
    "\n",
    "        #initalized the lstm with zeros (empty memory)\n",
    "        self.cx = torch.zeros(1,hidden1)\n",
    "        self.hx = torch.zeros(1,hidden1)\n",
    "\n",
    "    \n",
    "    def reset_lstm(self, done=True):\n",
    "        # reset the data held in the lstm, should be done everytime the simulation is restarted\n",
    "        # or after every batch\n",
    "        if done:\n",
    "            self.cx = self.cx = torch.zeros(1,hidden1)\n",
    "            self.hx = torch.zeros(1,hidden1)\n",
    "\n",
    "        else:\n",
    "            self.cx = torch.Tensor(self.cx.data)\n",
    "            self.hx = torch.Tensor(self.cx.data)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_states=None):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fcn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if hidden_states == None:\n",
    "            hx, cx = self.lstm(x.reshape(self.hx.shape), (self.hx, self.cx))\n",
    "        else:\n",
    "            hx, cx = self.lstm(x, hidden_states)\n",
    "\n",
    "        self.hx = hx\n",
    "        self.cx = cx\n",
    "        x = hx\n",
    "\n",
    "        # add relu layer here?\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.fcn2(x)\n",
    "        x =self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        return x, (self.hx, self.cx)    "
   ]
  },
  {
   "source": [
    "## Critic Network\n",
    "\n",
    "Add the lstm layer before or after the action have been added??\n",
    "\n",
    "Here i'm going with after, makes more sense"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, action_space, init_w=3e-3, hidden1=400, hidden2=300, init_b=3e-4):\n",
    "        super(Critic, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.action_space = action_space\n",
    "        self.num_outputs = action_space.shape[0]\n",
    "\n",
    "        # Build the architecture of the critic\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden1)\n",
    "        self.fcn1 = nn.LayerNorm(hidden1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden1 + self.num_outputs, hidden2)\n",
    "        self.fcn2 = nn.LayerNorm(hidden2)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(hidden2, hidden2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1.weight.data = fan_in_init(self.fc1.weight)\n",
    "        self.fc1.bias.data = fan_in_init(self.fc1.bias)\n",
    "\n",
    "        self.fc2.weight.data = fan_in_init(self.fc2.weight)\n",
    "        self.fc2.bias.data = fan_in_init(self.fc2.bias)\n",
    "\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_b, init_b)\n",
    "\n",
    "        #initalized the lstm with zeros (empty memory)\n",
    "        self.cx = torch.zeros(1,hidden2)\n",
    "        self.hx = torch.zeros(1,hidden2)\n",
    "\n",
    "    def forward(self, state, action, hidden_states=None):\n",
    "        x = state\n",
    "        x = self.fc1(x)\n",
    "        x = self.fcn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # TODO: changed the axis form 1 to 0, seems to work, might cause issues later\n",
    "        x = self.fc2(torch.cat([x, action], 0))\n",
    "        x = self.fcn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if hidden_states == None:\n",
    "            hx, cx = self.lstm(x.reshape(self.hx.shape), (self.hx, self.cx))\n",
    "        else:\n",
    "            hx, cx = self.lstm(x, hidden_states)\n",
    "\n",
    "        self.hx = hx\n",
    "        self.cx = cx\n",
    "        x = hx\n",
    "\n",
    "        out = self.fc3(x)\n",
    "        return out, (self.hx, self.cx)\n",
    "\n",
    "    def reset_lstm(self, done=True):\n",
    "        # reset the data held in the lstm, should be done everytime the simulation is restarted\n",
    "        # or after every batch\n",
    "        if done:\n",
    "            self.cx = self.cx = torch.zeros(1,hidden1)\n",
    "            self.hx = torch.zeros(1,hidden1)\n",
    "\n",
    "        else:\n",
    "            self.cx = torch.Tensor(self.cx.data)\n",
    "            self.hx = torch.Tensor(self.cx.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDPG(object):\n",
    "\n",
    "    def __init__(self, num_inputs, action_space, checkpoint_dir=None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = action_space.shape[0]\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr_actor = 10e-4\n",
    "        self.lr_critic = 10e-3\n",
    "        self.buffer_size = 10e6\n",
    "        self.batch_size = 64\n",
    "        self.noise_mean = np.zeros(self.num_outputs)\n",
    "        self.tau = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.weight_decay = 0.01\n",
    "\n",
    "        # create actor critic networks\n",
    "        self.actor = Actor(self.num_inputs, action_space)\n",
    "        self.critic = Critic(self.num_inputs, action_space)\n",
    "\n",
    "        # create target networks\n",
    "        self.target_actor = Actor(self.num_inputs, action_space)\n",
    "        self.target_critic = Critic(self.num_inputs, action_space)\n",
    "\n",
    "        # ensure that the weights of the targets are the same as the actor critic\n",
    "        hard_update(self.target_actor, self.actor)\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "\n",
    "        # set up the optimizers\n",
    "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=self.lr_critic,\n",
    "                                             weight_decay=self.weight_decay)\n",
    "\n",
    "        # create replay buffer and noise\n",
    "        self.buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "        self.noise = ActionNoise(self.noise_mean)\n",
    "\n",
    "        # Set the directory to save the models\n",
    "        if checkpoint_dir is None:\n",
    "            self.checkpoint_dir = \"./rdpg_saves/\"\n",
    "        else:\n",
    "            self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def update(self, hx, cx):\n",
    "        batch = Transition(*zip(*self.buffer.sample()))\n",
    "\n",
    "        state_batch = torch.cat(batch.state).float()\n",
    "        action_batch = torch.cat(batch.action).float()\n",
    "        reward_batch = torch.cat(batch.reward).float()\n",
    "        done_batch = torch.cat(batch.done).float()\n",
    "        next_state_batch = torch.cat(batch.next_state).float()\n",
    "\n",
    "        next_action, (next_hx, next_cx) = self.target_actor(next_state_batch, (hx, cx))\n",
    "        next_q, (h1, c1) = self.target_critic(next_state_batch, next_action.detach(), (hx, cx))\n",
    "\n",
    "        reward_batch = reward_batch.unsqueeze(1)\n",
    "        done_batch = done_batch.unsqueeze(1)\n",
    "        exp_values = reward_batch + (1 - done_batch) * self.gamma * next_qs\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        state_action_batch = self.critic(state_batch, action_batch, (next_hx, next_cx))\n",
    "        value_loss = F.mse_loss(state_action_batch, exp_values.detach())\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()   \n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        policy_loss = - self.critic(state_batch, self.actor(state_batch), (next_hx, next_cx))\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()    \n",
    "\n",
    "        soft_update(self.target_actor, self.actor, self.tau)\n",
    "        soft_update(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "        return value_loss.item(), policy_loss.item(), (next_hx, next_cx)\n",
    "    \n",
    "    def get_action(self, state, add_noise=True):\n",
    "        self.actor.eval()  # puts actor into evaluation mode, ie not training any more, this means for eg that dropout layers dont dropout etc\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up â€¦\n",
    "            mu = self.actor(state)[0]\n",
    "\n",
    "        self.actor.train()  # return actor to train mode, undos eval mode\n",
    "\n",
    "        if add_noise:\n",
    "            mu += self.noise()\n",
    "        # return np.clip(mu, -1 ,1)\n",
    "        return mu.clamp(self.action_space.low[0], self.action_space.high[0])\n",
    "\n",
    "    def random_action(self):\n",
    "        action = np.random.uniform(-1, 1, self.num_inputs)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def set_eval(self):\n",
    "        # set all agents to evaluation mode\n",
    "        self.actor.eval()\n",
    "        self.critic.eval()\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "\n",
    "    def set_train(self):\n",
    "        # set all agents to training mode\n",
    "        self.actor.train()\n",
    "        self.critic.train()\n",
    "        self.target_actor.train()\n",
    "        self.target_critic.train()\n",
    "\n",
    "    def save(self, last_time):\n",
    "        save_path = self.checkpoint_dir + f'/ep{last_time}.pth.tar'\n",
    "        print('Saving...')\n",
    "        checkpoint = {\n",
    "            'last_timestep': last_time,\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'actor_target': self.target_actor.state_dict(),\n",
    "            'critic_target': self.target_critic.state_dict(),\n",
    "            'actor_optim': self.actor_optim.state_dict(),\n",
    "            'critic_optim': self.critic_optim.state_dict(),\n",
    "            'memory': self.memory\n",
    "        }\n",
    "        torch.save(checkpoint, save_path)\n",
    "        # Garbage collection, reclaims some memory\n",
    "        gc.collect()\n",
    "        print(f\"Model saved: {last_time},  {save_path}\")\n",
    "\n",
    "    def load(self, path=None):\n",
    "        # Loads checkpoint\n",
    "        if path is None:\n",
    "            path = self.get_path()\n",
    "\n",
    "        if os.path.isfile(path):\n",
    "            print(\"Loading checkpoint...\")\n",
    "\n",
    "        checkpoint = torch.load(path)\n",
    "        timestep = checkpoint['last_timestep'] + 1\n",
    "\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.target_actor.load_state_dict(checkpoint['actor_target'])\n",
    "        self.target_critic.load_state_dict(checkpoint['critic_target'])\n",
    "        self.actor_optim.load_state_dict(checkpoint['actor_optim'])\n",
    "        self.critic_optim.load_state_dict(checkpoint['critic_optim'])\n",
    "        replay_buffer = checkpoint['memory']\n",
    "\n",
    "        gc.collect()\n",
    "        print('Model Loaded')\n",
    "        return timestep, replay_buffer\n",
    "\n",
    "    def get_path(self):\n",
    "        # Gets the path of the latest file\n",
    "        files = [file for file in os.listdir(self.checkpoint_dir) if (file.endswith(\".pt\") or file.endswith(\"tar\"))]\n",
    "        path = [os.path.join(self.checkpoint_dir, file) or file in files]\n",
    "        last_file = max(path, key=os.path.getctime)\n",
    "        return os.path.abspath(last_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_Boeing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('boeing-danger-v0')\n",
    "# env = NormalizedEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RDPG(3, action_space=env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_cycles = 10\n",
    "warmup = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Parameter ``U``: Wrong element data type: 'object'. Array elements must be numbers.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-9ade8174ddcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fotin\\onedrive - university of cambridge\\university - personal\\engineering iib\\project\\code\\masters_project\\env_boeing\\gym-boeing\\gym_Boeing\\envs\\boeing_danger.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# sq_error = np.sum(np.square(self.observation))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fotin\\onedrive - university of cambridge\\university - personal\\engineering iib\\project\\code\\masters_project\\env_boeing\\gym-boeing\\utils\\flight.py\u001b[0m in \u001b[0;36mio\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;34m\"\"\"Input Output for the system\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0myout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlsim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\FOTIN\\master_venv\\lib\\site-packages\\control\\matlab\\timeresp.py\u001b[0m in \u001b[0;36mlsim\u001b[1;34m(sys, U, T, X0)\u001b[0m\n\u001b[0;32m    271\u001b[0m     '''\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeresp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforced_response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m     \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforced_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0myout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\FOTIN\\master_venv\\lib\\site-packages\\control\\timeresp.py\u001b[0m in \u001b[0;36mforced_response\u001b[1;34m(sys, T, U, X0, transpose, interpolate, squeeze)\u001b[0m\n\u001b[0;32m    339\u001b[0m             U = _check_convert_array(U, legal_shapes,\n\u001b[0;32m    340\u001b[0m                                      \u001b[1;34m'Parameter ``U``: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                                      transpose=transpose)\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# convert 1D array to 2D array with only one row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\FOTIN\\master_venv\\lib\\site-packages\\control\\timeresp.py\u001b[0m in \u001b[0;36m_check_convert_array\u001b[1;34m(in_obj, legal_shapes, err_msg_start, squeeze, transpose)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Wrong element data type: '{d}'. Array elements \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                   \u001b[1;34m\"must be numbers.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg_start\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m# If array is zero dimensional (in_obj is scalar):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Parameter ``U``: Wrong element data type: 'object'. Array elements must be numbers."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "agent.set_train()\n",
    "\n",
    "timestep = 1\n",
    "rewards, policy_losses, value_losses, mean_test_rewards = [], [], [], []\n",
    "epoch = 0\n",
    "t = 0\n",
    "time_last_checkpoint = time.time()\n",
    "\n",
    "while timestep <= 100:\n",
    "    agent.noise.reset()\n",
    "    epoch_return = 0.\n",
    "    state = torch.Tensor([env.reset()])\n",
    "\n",
    "    hx = agent.actor.hx\n",
    "    cx = agent.actor.cx\n",
    "\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        print(env.step(action.numpy()))\n",
    "        next_state, reward, done, _ = env.step(action.numpy())\n",
    "        \n",
    "        print(done, reward, _)\n",
    "        timestep += 1\n",
    "        epoch_return += reward\n",
    "\n",
    "        mask = torch.Tensor([done])\n",
    "        reward = torch.Tensor([reward])\n",
    "        next_state = torch.Tensor([next_state])\n",
    "\n",
    "        agent.buffer.add(state, action, mask, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        epoch_value_loss = 0\n",
    "        epoch_policy_loss = 0\n",
    "\n",
    "        if len(agent.buffer) > agent.buffer.mini_batch_size:\n",
    "            value_loss, policy_loss, (hx, cx) = agent.update(hx, cx)\n",
    "\n",
    "            epoch_value_loss += value_loss\n",
    "            epoch_policy_loss += policy_loss\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(epoch_return)\n",
    "    value_losses.append(epoch_value_loss)\n",
    "    policy_losses.append(epoch_policy_loss)\n",
    "\n",
    "    if timestep >= 10 * t:\n",
    "        print('Epoch:', epoch)\n",
    "        t += 1\n",
    "        test_rewards = []\n",
    "\n",
    "        for _ in range(n_test_cycles):\n",
    "            state = torch.Tensor(env.reset())\n",
    "            test_reward = 0\n",
    "            while True:\n",
    "                # this is a bit different form the implementation used above, although it does the same job\n",
    "                # this is due to a bug that instead of returning action:[[]], returns action:[] needing for\n",
    "                # the action to be reshaped\n",
    "                action = agent.get_action(state, add_noise=False)\n",
    "                action = action.numpy()\n",
    "                action = action.reshape((2,))\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                print(done, _)\n",
    "                test_reward += reward\n",
    "                next_state = torch.Tensor([next_state])\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            test_rewards.append(test_reward)\n",
    "        mean_test_rewards.append(np.mean(test_reward))\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    # save model\n",
    "    agent.save(timestep, agent.buffer.memory)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}