{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Recurent Deterministic Policy Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "RDPG is a natural extension of the DDPG, where an some recurrency is added to the model, so that it can use and adjust using past events. \n",
    "\n",
    "Especially beneficial in the case of partially observed enviroments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The changes from the DDPG architecture seem to limit themselves to the memory buffer and the actor-critic architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Action Noise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActionNoise:\n",
    "\n",
    "    def __init__(self, mu, theta=0.15, sigma=0.2, x0=None, dt=0.05):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.mu = mu  # will be initialized as a list of zeros\n",
    "        self.x0 = x0\n",
    "        self.dt = dt  # same as flight model\n",
    "\n",
    "        self.reset()  # sets x_prev\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(\n",
    "            self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "source": [
    "## Replay Buffer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'done', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, size, mini_batch_size):\n",
    "        self.size = size\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, *args):\n",
    "        \"\"\"Add transition to buffer\"\"\"\n",
    "\n",
    "        # This is a strange way to implement it, but makes it FIFO efficiently\n",
    "        if len(self.memory) < self.size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = int((self.position + 1) % self.size)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Get a minibatch from buffer\"\"\"\n",
    "        return random.sample(self.memory, self.mini_batch_size)\n",
    "\n",
    "    def sample_trajectory(self):\n",
    "        \"\"\"Get a random batch of consecutive results\"\"\"\n",
    "        start = random.randint(0, self.size-self.mini_batch_size)\n",
    "        return self.memory[start :start+self.mini_batch_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Memory buffer used for learning, takes in a tuple: ('state', 'action', 'done', 'next_state', 'reward')\"\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Fan-In Initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def fan_in_init(tensor, fan_in=None):\n",
    "    # Either of the above inputs works\n",
    "\n",
    "    if fan_in is None:\n",
    "        fan_in = tensor.size(-1)\n",
    "\n",
    "    w = 1. / np.sqrt(fan_in)\n",
    "    return nn.init.uniform_(tensor, -w, w)"
   ]
  },
  {
   "source": [
    "## Enviroment Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def step(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low) / 2\n",
    "        act_b = (self.action_space.high + self.action_space.low) / 2\n",
    "\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2. / (self.action_space.high - self.action_space.low)\n",
    "        act_b_inv = (self.action_space.high + self.action_space.low) / 2\n",
    "        return act_k_inv * (action - act_b_inv)"
   ]
  },
  {
   "source": [
    "## Actor Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self,num_inputs, action_space, init_w=3e-3, init_b=3e-4, hidden1=400, hidden2=300):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.num_ouptuts = action_space.shape[0]\n",
    "\n",
    "        # original model has lstm after second layer, but i think it might work better between hidden1 and hidden2\n",
    "\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden1)\n",
    "        self.fcn1 = nn.LayerNorm(hidden1)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(hidden1, hidden1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fcn2 = nn.LayerNorm(hidden2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden2, self.num_ouptuts)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # initialize weights\n",
    "        self.fc1.weight.data = fan_in_init(self.fc1.weight)\n",
    "        self.fc1.bias.data = fan_in_init(self.fc1.bias)\n",
    "\n",
    "        self.fc2.weight.data = fan_in_init(self.fc2.weight)\n",
    "        self.fc2.bias.data = fan_in_init(self.fc2.bias)\n",
    "\n",
    "        # The final layer weights and biases were initialized from uniform [-3e-3, 3e-3]\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_b, init_b)\n",
    "\n",
    "        #initalized the lstm with zeros (empty memory)\n",
    "        self.cx = torch.zeros(1,hidden1)\n",
    "        self.hx = torch.zeros(1,hidden1)\n",
    "\n",
    "    \n",
    "    def reset_lstm(self, done=True):\n",
    "        # reset the data held in the lstm, should be done everytime the simulation is restarted\n",
    "        # or after every batch\n",
    "        if done:\n",
    "            self.cx = self.cx = torch.zeros(1,hidden1)\n",
    "            self.hx = torch.zeros(1,hidden1)\n",
    "\n",
    "        else:\n",
    "            self.cx = torch.Tensor(self.cx.data)\n",
    "            self.hx = torch.Tensor(self.cx.data)\n",
    "\n",
    "\n",
    "    def forward(self, x, , hidden_states=None):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fcn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if hidden_states == None:\n",
    "            hx, cx = self.lstm(x.reshape(self.hx.shape), (self.hx, self.cx))\n",
    "        else:\n",
    "            hx, cx = self.lstm(x, hidden_states)\n",
    "\n",
    "        self.hx = hx\n",
    "        self.cx = cx\n",
    "        x = hx\n",
    "\n",
    "        # add relu layer here?\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.fcn2(x)\n",
    "        x =self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        return x, (self.hx, self.cx)    "
   ]
  },
  {
   "source": [
    "## Critic Network\n",
    "\n",
    "Add the lstm layer before or after the action have been added??\n",
    "\n",
    "Here i'm going with after, makes more sense"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, action_space, init_w=3e-3, hidden1=400, hidden2=300, init_b=3e-4):\n",
    "        super(Critic, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.action_space = action_space\n",
    "        self.num_outputs = action_space.shape[0]\n",
    "\n",
    "        # Build the architecture of the critic\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden1)\n",
    "        self.fcn1 = nn.LayerNorm(hidden1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden1 + self.num_outputs, hidden2)\n",
    "        self.fcn2 = nn.LayerNorm(hidden2)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(hidden2, hidden2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1.weight.data = fan_in_init(self.fc1.weight)\n",
    "        self.fc1.bias.data = fan_in_init(self.fc1.bias)\n",
    "\n",
    "        self.fc2.weight.data = fan_in_init(self.fc2.weight)\n",
    "        self.fc2.bias.data = fan_in_init(self.fc2.bias)\n",
    "\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.fc3.bias.data.uniform_(-init_b, init_b)\n",
    "\n",
    "        #initalized the lstm with zeros (empty memory)\n",
    "        self.cx = torch.zeros(1,hidden2)\n",
    "        self.hx = torch.zeros(1,hidden2)\n",
    "\n",
    "    def forward(self, state, action, hidden_states=None):\n",
    "        x = state\n",
    "        x = self.fc1(x)\n",
    "        x = self.fcn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # TODO: changed the axis form 1 to 0, seems to work, might cause issues later\n",
    "        x = self.fc2(torch.cat([x, action], 0))\n",
    "        x = self.fcn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if hidden_states == None:\n",
    "            hx, cx = self.lstm(x.reshape(self.hx.shape), (self.hx, self.cx))\n",
    "        else:\n",
    "            hx, cx = self.lstm(x, hidden_states)\n",
    "\n",
    "        self.hx = hx\n",
    "        self.cx = cx\n",
    "        x = hx\n",
    "\n",
    "        out = self.fc3(x)\n",
    "        return out, (self.hx, self.cx)\n",
    "\n",
    "    def reset_lstm(self, done=True):\n",
    "        # reset the data held in the lstm, should be done everytime the simulation is restarted\n",
    "        # or after every batch\n",
    "        if done:\n",
    "            self.cx = self.cx = torch.zeros(1,hidden1)\n",
    "            self.hx = torch.zeros(1,hidden1)\n",
    "\n",
    "        else:\n",
    "            self.cx = torch.Tensor(self.cx.data)\n",
    "            self.hx = torch.Tensor(self.cx.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDPG(object):\n",
    "\n",
    "    def __init__(self, num_inputs, action_space, checkpoint_dir=None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = action_space.shape[0]\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr_actor = 10e-4\n",
    "        self.lr_critic = 10e-3\n",
    "        self.buffer_size = 10e6\n",
    "        self.batch_size = 64\n",
    "        self.noise_mean = np.zeros(self.num_outputs)\n",
    "        self.tau = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.weight_decay = 0.01\n",
    "\n",
    "        # create actor critic networks\n",
    "        self.actor = Actor(self.num_inputs, action_space)\n",
    "        self.critic = Critic(self.num_inputs, action_space)\n",
    "\n",
    "        # create target networks\n",
    "        self.target_actor = Actor(self.num_inputs, action_space)\n",
    "        self.target_critic = Critic(self.num_inputs, action_space)\n",
    "\n",
    "        # ensure that the weights of the targets are the same as the actor critic\n",
    "        hard_update(self.target_actor, self.actor)\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "\n",
    "        # set up the optimizers\n",
    "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=self.lr_critic,\n",
    "                                             weight_decay=self.weight_decay)\n",
    "\n",
    "        # create replay buffer and noise\n",
    "        self.buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "        self.noise = ActionNoise(self.noise_mean)\n",
    "\n",
    "        # Set the directory to save the models\n",
    "        if checkpoint_dir is None:\n",
    "            self.checkpoint_dir = \"./rdpg_saves/\"\n",
    "        else:\n",
    "            self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def update(self):\n",
    "        batch = Transition(*zip(*self.buffer.sample()))\n",
    "\n",
    "        state_batch = torch.cat(batch.state).float()\n",
    "        action_batch = torch.cat(batch.action).float()\n",
    "        reward_batch = torch.cat(batch.reward).float()\n",
    "        done_batch = torch.cat(batch.done).float()\n",
    "        next_state_batch = torch.cat(batch.next_state).float()\n",
    "\n",
    "        # TODO: here I need to find a way to link that memory of the actor and the critic (maybe this should be done in the actor critic network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}